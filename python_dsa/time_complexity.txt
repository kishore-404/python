# ✅ Definition:
# Time Complexity is a way to measure how the **runtime of an algorithm grows** 
# with the size of the input (denoted as 'n').
# It is expressed using **Big O notation**.

# ✅ Why we need Time Complexity:
# 🔹 To compare the **efficiency** of algorithms
# 🔹 To avoid writing **slow code** for large inputs
# 🔹 To prepare for coding interviews and system design
# 🔹 Helps make better **performance decisions**

# -------------------------------------------------------

# ✅ Common Big O Time Complexities:

# 🔸 O(1) — Constant time
#     Always takes the same amount of time regardless of input size
# 🔸 O(log n) — Logarithmic time
#     Input size is divided each time (e.g., binary search)
# 🔸 O(n) — Linear time
#     Time increases proportionally with input size
# 🔸 O(n log n) — Linearithmic time
#     Common in efficient sorting algorithms (Merge Sort, Quick Sort)
# 🔸 O(n²) — Quadratic time
#     Often found in nested loops (Bubble Sort, brute-force algorithms)
# 🔸 O(2ⁿ), O(n!) — Exponential & Factorial time
#     Extremely slow; used in complex recursive problems (like backtracking)

# -------------------------------------------------------

# ✅ Example 1: Constant Time - O(1)

def get_first_element(lst):
    return lst[0]

# No matter the size of lst, it takes 1 step

# -------------------------------------------------------

# ✅ Example 2: Linear Time - O(n)

def print_all(lst):
    for item in lst:
        print(item)

# As the input size grows, the loop runs more times

# -------------------------------------------------------

# ✅ Example 3: Quadratic Time - O(n²)

def print_pairs(lst):
    for i in lst:
        for j in lst:
            print(i, j)

# Two nested loops over the same list → n * n = n²

# -------------------------------------------------------

# ✅ Visualization (for intuition):
# Input size n = 5
# O(1) → 1 step
# O(n) → 5 steps
# O(n²) → 25 steps
# O(2ⁿ) → 32 steps

# -------------------------------------------------------

# ✅ Time Complexity vs. Space Complexity:

# Time Complexity → How long it takes to run
# Space Complexity → How much memory it uses

# Often a trade-off: faster algorithms may use more memory and vice versa.

# -------------------------------------------------------

# ✅ How to calculate time complexity:
# 🔹 Count number of basic operations (in worst-case scenario)
# 🔹 Focus on loops, recursion, nested calls
# 🔹 Drop constants and less significant terms (e.g., O(3n + 5) → O(n))

# -------------------------------------------------------

# ✅ Benefits of analyzing time complexity:
# 🔸 Predict performance before running the code
# 🔸 Improve scalability for large datasets
# 🔸 Compare algorithms meaningfully

# -------------------------------------------------------

# ✅ Limitations:
# ❌ Does not measure actual execution time
# ❌ Assumes worst-case unless stated otherwise
# ❌ Doesn't consider real-world factors like hardware, caching, etc.

